# HSM3D
HSM3D: Hierarchical Superpoint Mamba for LiDAR Scene Point Cloud Semantic Segmentation

LiDAR point cloud semantic segmentation plays a fundamental role in Intelligent Transportation Systems (ITS), serving as a key perception module for applications such as High-Definition (HD) map construction, autonomous driving, and large-scale road infrastructure understanding. However, urban-scale LiDAR scenes containing hundreds of millions of points still pose significant challenges to existing methods in terms of segmentation accuracy, scalability, and computational efficiency. Recently, selective state space models (Mamba) have shown strong potential for large-scale point cloud processing, offering powerful long-range dependency modeling with linear complexity. This paper presents HSM3D, a novel Hierarchical Superpoint Mamba framework
designed for LiDAR scene point cloud semantic segmentation
to support ITS.
HSM3D introduces a recurrent-style superpoint architecture that outperforms conventional cascaded superpoint networks in segmentation accuracy. An Adaptive multi-scale Superpoint Generation (ASG) module in HSM3D enables precise point cloud clustering of complex outdoor scenes and reduces input token size for better scalability. The Sequence-Robust Mamba (SRM) block refines superpoint features by recurrently modeling the sparse random sequence within each superpoint. Extensive experiments on urban and street datasets (Toronto-3D, DALES, and MS-LiDAR) demonstrate that HSM3D achieves State-Of-The-Art (SOTA) segmentation accuracy while improving computational efficiency. It delivers 2-4x faster model inference (excluding preprocessing time) and uses only half the GPU memory of recent baselines, confirming its effectiveness and scalability for large-scale point cloud segmentation.

<img width="1692" height="750" alt="ff15f1f5-1620-4f26-9ad3-bbb8d216f25d" src="https://github.com/user-attachments/assets/22480357-1509-42fe-91f1-702b9db3d7f8" />
